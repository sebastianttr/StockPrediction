{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reuters Data fetch and save\n",
    "\n",
    "This notebook downloads all the data from the reuters about a specific topic and saves it to the database"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0dcebd334270cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import math\n",
    "import ray\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os.path\n",
    "from time import sleep\n",
    "from dateutil import parser\n",
    "import asyncio\n",
    "from pyppeteer import launch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T16:55:08.913761Z",
     "start_time": "2024-06-21T16:55:08.485181Z"
    }
   },
   "id": "4194653680e1e8be",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "class NewsMetaRepository:\n",
    "    def __init__(self, csv_file=None):\n",
    "        self.connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "        if os.path.isfile(f'{csv_file}/news_meta.csv'):\n",
    "            try:\n",
    "                self.connection.execute(f\"IMPORT DATABASE '{csv_file}';\")\n",
    "            except Exception as e:\n",
    "                self._create_table()\n",
    "                self.connection.execute(f\"EXPORT DATABASE '{csv_file}';\")\n",
    "        else:\n",
    "            self._create_table()\n",
    "\n",
    "    def _create_table(self):\n",
    "        self.connection.execute(\"CREATE TABLE news_meta (id VARCHAR, title VARCHAR UNIQUE, url VARCHAR, timestamp VARCHAR UNIQUE,term VARCHAR)\")\n",
    "        self.connection.execute(\"CREATE SEQUENCE id_sequence START 1 INCREMENT BY 1;\")\n",
    "\n",
    "    def insert(self, news_meta):\n",
    "        self.connection.execute(\"PREPARE insert_meta AS \"\n",
    "                                \"INSERT INTO news_meta VALUES (nextval('id_sequence'), ?, ?, ?, ?) ON CONFLICT DO NOTHING;\")\n",
    "        self.connection.execute(f\"EXECUTE insert_meta('{news_meta['title']}', '{news_meta['url']}', '{news_meta['timestamp']}', '{news_meta['term']}');\")\n",
    "\n",
    "    def select_all(self):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta\").fetchdf()\n",
    "\n",
    "    def select_by_id(self, id):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE id = ?\", id).fetchdf()\n",
    "\n",
    "    def select_by_title(self, title):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE title = ?\", title).fetchdf()\n",
    "\n",
    "    def select_by_url(self, url):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE url = ?\", url).fetchdf()\n",
    "\n",
    "    def select_by_term(self, source):\n",
    "        self.connection.execute(\"PREPARE select_by_source AS \"\n",
    "                                \"SELECT * FROM news_meta WHERE term = ?\")\n",
    "        return self.connection.execute(f\"EXECUTE select_by_source('{source}');\").fetchdf()\n",
    "\n",
    "    def select_by_date(self, date_from, date_to):\n",
    "\n",
    "        return self.connection.execute(f\"SELECT * FROM news_meta WHERE strptime(timestamp, '%Y-%m-%dT%H:%M:%S%z') BETWEEN strptime('{date_from}','%Y-%m-%d') AND strptime('{date_to}','%Y-%m-%d')\").fetchdf()\n",
    "\n",
    "    def select_by_date_and_term(self, date_from, date_to, term):\n",
    "        print(date_from, date_to, term)\n",
    "        return self.connection.execute(f\"SELECT * FROM news_meta WHERE term = '{term}' AND strptime(timestamp, '%Y-%m-%dT%H:%M:%S%z') BETWEEN strptime('{date_from}','%Y-%m-%d') AND strptime('{date_to}','%Y-%m-%d')\").fetchdf()\n",
    "\n",
    "    def delete_all(self):\n",
    "        self.connection.execute(\"DELETE FROM news_meta\")\n",
    "\n",
    "    def export(self, csv_file):\n",
    "        self.connection.execute(f\"EXPORT DATABASE '{csv_file}';\")\n",
    "\n",
    "    def close(self):\n",
    "        self.connection.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:39.317333Z",
     "start_time": "2024-06-21T15:28:39.247685Z"
    }
   },
   "id": "f042538ad378208b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_PAGE_SIZE = 50\n",
    "REUTERS_QUERY_URL = 'https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2'\n",
    "PAGES_REQUESTS_SIZE = 5    # the amount of request ran in parallel to get pages faster, 10 pages loaded at the same time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:39.322237Z",
     "start_time": "2024-06-21T15:28:39.318160Z"
    }
   },
   "id": "c57473e637439ebb",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 17:28:41,074\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265 \u001B[39m\u001B[22m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "This functions queries reuters and gets the first few articles\n",
    "This query contains the \n",
    "'''\n",
    "\n",
    "num_cpus = 4\n",
    "    \n",
    "ray.init(ignore_reinit_error=True, num_cpus=num_cpus)\n",
    "\n",
    "newsMetaRepo = NewsMetaRepository(\"meta/reuters\")\n",
    "\n",
    "def get_query_param(keyword, offset):    \n",
    "    return {\n",
    "        \"query\": '{\"keyword\":\"%s\",\"offset\":%d,\"orderby\":\"display_date:desc\",\"size\":%d,\"website\":\"reuters\"}' % (keyword, offset, MAX_PAGE_SIZE),\n",
    "        \"d\": 201,\n",
    "        \"_website\": \"reuters\"\n",
    "    }\n",
    "\n",
    "def insert_article_list_to_db(article_list, term):\n",
    "    requests_keys = [\"id\",\"canonical_url\",\"title\",\"published_time\"]\n",
    "\n",
    "    for query_articles in article_list:\n",
    "        request_values = {key: query_articles[key] for key in requests_keys}\n",
    "        \n",
    "        news_meta = {\n",
    "            'title': request_values['title'].replace(\"'\",\"''\"),\n",
    "            'term': term,\n",
    "            'timestamp': request_values['published_time'],\n",
    "            'url': request_values['canonical_url']\n",
    "        }\n",
    "        newsMetaRepo.insert(news_meta)\n",
    "\n",
    "# resources = {\"requests_resource\": 1}, num_cpus=num_cpus\n",
    "@ray.remote\n",
    "def try_request(url, headers, params):\n",
    "    response_page = requests.request(\"GET\", url, headers=headers, params=params, timeout=30)\n",
    "    try:\n",
    "        json_response = response_page.json()\n",
    "                \n",
    "        if response_page.status_code > 300:\n",
    "            print(\"Status code error: \" + str(response_page.status_code))\n",
    "            return False, params\n",
    "\n",
    "        return True, json_response\n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(f\"Bad Request: GET {url} \\n Status Code: {response_page.status_code} | Error : {e}\")\n",
    "        return False, params\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timed out\")\n",
    "        return False, params\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Bad Request: GET {url} \\n\")\n",
    "        return False, params\n",
    "\n",
    "def get_news_articles_list(keyword):\n",
    "\n",
    "    headers = {\n",
    "        'sec-ch-ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\"',\n",
    "        'DNT': '1',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua-arch': '\"arm\"',\n",
    "        'sec-ch-device-memory': '8',\n",
    "        'Referer': 'https://www.reuters.com/site-search/?query=Tesla&offset=0',\n",
    "        'sec-ch-ua-full-version-list': '\"Not/A)Brand\";v=\"8.0.0.0\", \"Chromium\";v=\"126.0.6478.57\"',\n",
    "        'sec-ch-ua-model': '\"\"',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'Cookie': 'datadome=5DoGfdp_v7Y64hF3yeg8zmFRJvUZ_kY9APCBHhhY8Q5HnPWurio8XinFbT~guJr5RlIj0N41S2aQQNBEiMf05rg2Mb3BBC9zMM7Akw1aS8O9285Y~CC1YlyPQoYKfgLi; reuters-geo={\"country\":\"AT\", \"region\":\"-\"}'\n",
    "    }\n",
    "   \n",
    "    pages_range = None\n",
    "    last_offset = 0\n",
    "    f = IntProgress(min=0, max=10) # instantiate the bar\\n\",\n",
    "    display(f) # display the bar\n",
    "    \n",
    "    hasResults = True\n",
    "    page_idx = 0\n",
    "    \n",
    "    while hasResults:\n",
    "\n",
    "        tasks = []\n",
    "        for _ in range(PAGES_REQUESTS_SIZE):\n",
    "            offset = MAX_PAGE_SIZE + last_offset\n",
    "            tasks.append(\n",
    "                try_request.remote(\n",
    "                    REUTERS_QUERY_URL, \n",
    "                    params = get_query_param(keyword=keyword, offset=offset), \n",
    "                    headers = headers)\n",
    "            )\n",
    "            last_offset = offset\n",
    "        \n",
    "        results_list = ray.get(tasks)\n",
    "        \n",
    "        for (request_ok, json_parsed) in results_list:\n",
    "            if request_ok:\n",
    "                if pages_range is None:\n",
    "                    query_result_total_size = json_parsed[\"result\"][\"pagination\"][\"total_size\"]\n",
    "                    total_page_size = math.ceil(query_result_total_size / MAX_PAGE_SIZE)\n",
    "                    pages_range = math.ceil(total_page_size / PAGES_REQUESTS_SIZE)\n",
    "                    f.max = pages_range\n",
    "                \n",
    "                try:\n",
    "                    # print(len(json_parsed[\"result\"][\"articles\"]))\n",
    "                    if(json_parsed[\"result\"] is not None):\n",
    "                        insert_article_list_to_db(json_parsed[\"result\"][\"articles\"], keyword)\n",
    "                    else:\n",
    "                        hasResults = False\n",
    "                except Exception as e:\n",
    "                    hasResults = False\n",
    "                    print(f\"Error getting articles! {str(json_parsed)}\")\n",
    "                    pass\n",
    "            else:\n",
    "                print(f\"Error at task {last_offset}\")\n",
    "            sleep(0.5)\n",
    "\n",
    "        print(\"\\rLoading page %d / %d\" % (page_idx + 1, pages_range if pages_range is not None else 0), end=\"\")\n",
    "\n",
    "        page_idx += 1\n",
    "        f.value = page_idx\n",
    "        newsMetaRepo.export(\"meta/reuters\")\n",
    "        sleep(1)\n",
    "\n",
    "# get_news_articles_list(\"Tesla\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:41.484381Z",
     "start_time": "2024-06-21T15:28:39.339238Z"
    }
   },
   "id": "1c222d877add03eb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 , https://www.reuters.com/business/autos-transportation/china-based-ev-makers-hit-with-european-union-tariffs-2024-06-12/\n",
      "Error downloading article 1 | 401\n",
      "Downloading 2 , https://www.reuters.com/sustainability/decarbonizing-industries/ev-sales-slip-can-auto-industry-navigate-bumps-road-net-zero-2024-06-12/\n",
      "Error downloading article 2 | 401\n",
      "Downloading 3 , https://www.reuters.com/legal/tesla-shareholder-sues-musk-return-billions-alleged-unlawful-profits-2024-06-11/\n",
      "Error downloading article 3 | 401\n",
      "Downloading 4 , https://www.reuters.com/business/autos-transportation/chinas-nio-says-commitment-europe-ev-market-unwavering-despite-increased-tariffs-2024-06-12/\n",
      "Error downloading article 4 | 401\n",
      "Downloading 5 , https://www.reuters.com/sustainability/climate-energy/orsted-install-tesla-battery-uk-offshore-wind-farm-2024-06-12/\n",
      "Error downloading article 5 | 401\n",
      "Downloading 6 , https://www.reuters.com/business/autos-transportation/swedish-fund-manager-vote-against-musks-56-bln-tesla-pay-package-2024-06-11/\n",
      "Error downloading article 6 | 401\n",
      "Downloading 7 , https://www.reuters.com/business/autos-transportation/what-will-happen-tesla-ceo-elon-musks-56-billion-pay-package-2024-06-11/\n",
      "Error downloading article 7 | 401\n",
      "Downloading 8 , https://www.reuters.com/business/autos-transportation/some-nordic-retail-brokers-allow-clients-vote-tesla-agm-2024-06-11/\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCancelledError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 90\u001B[0m\n\u001B[1;32m     86\u001B[0m             sleep(\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     88\u001B[0m         sleep(\u001B[38;5;241m0.300\u001B[39m)\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m load_article(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesla\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[6], line 55\u001B[0m, in \u001B[0;36mload_article\u001B[0;34m(search_term)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;28mlen\u001B[39m(news_meta_list)):\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# response = requests.request(\"GET\", url, headers=headers, data=payload)\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 55\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m download_to_database(\n\u001B[1;32m     56\u001B[0m             news_meta_list[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m][i],\n\u001B[1;32m     57\u001B[0m             news_meta_list[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m][i],\n\u001B[1;32m     58\u001B[0m             get_file_name(i)\n\u001B[1;32m     59\u001B[0m         )\n\u001B[1;32m     60\u001B[0m         sleep(\u001B[38;5;241m0.500\u001B[39m)\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# task_list = [\u001B[39;00m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;66;03m#     asyncio.create_task(\u001B[39;00m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;66;03m#         download_to_database(\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;66;03m# ]\u001B[39;00m\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;66;03m# await asyncio.gather(*task_list)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 19\u001B[0m, in \u001B[0;36mdownload_to_database\u001B[0;34m(news_meta_path, news_meta_id, filename)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m aiohttp\u001B[38;5;241m.\u001B[39mClientSession(headers\u001B[38;5;241m=\u001B[39mheaders) \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDownloading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnews_meta_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m , \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m session\u001B[38;5;241m.\u001B[39mget(url) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     21\u001B[0m             response_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext()\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/aiohttp/client.py:1194\u001B[0m, in \u001B[0;36m_BaseRequestContextManager.__aenter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1193\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__aenter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _RetType:\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coro\n\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resp\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/aiohttp/client.py:578\u001B[0m, in \u001B[0;36mClientSession._request\u001B[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001B[0m\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m ceil_timeout(\n\u001B[1;32m    574\u001B[0m         real_timeout\u001B[38;5;241m.\u001B[39mconnect,\n\u001B[1;32m    575\u001B[0m         ceil_threshold\u001B[38;5;241m=\u001B[39mreal_timeout\u001B[38;5;241m.\u001B[39mceil_threshold,\n\u001B[1;32m    576\u001B[0m     ):\n\u001B[1;32m    577\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connector \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 578\u001B[0m         conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connector\u001B[38;5;241m.\u001B[39mconnect(\n\u001B[1;32m    579\u001B[0m             req, traces\u001B[38;5;241m=\u001B[39mtraces, timeout\u001B[38;5;241m=\u001B[39mreal_timeout\n\u001B[1;32m    580\u001B[0m         )\n\u001B[1;32m    581\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mTimeoutError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ServerTimeoutError(\n\u001B[1;32m    583\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection timeout \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto host \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(url)\n\u001B[1;32m    584\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/aiohttp/connector.py:544\u001B[0m, in \u001B[0;36mBaseConnector.connect\u001B[0;34m(self, req, traces, timeout)\u001B[0m\n\u001B[1;32m    541\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m trace\u001B[38;5;241m.\u001B[39msend_connection_create_start()\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 544\u001B[0m     proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection(req, traces, timeout)\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed:\n\u001B[1;32m    546\u001B[0m         proto\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/aiohttp/connector.py:911\u001B[0m, in \u001B[0;36mTCPConnector._create_connection\u001B[0;34m(self, req, traces, timeout)\u001B[0m\n\u001B[1;32m    909\u001B[0m     _, proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_proxy_connection(req, traces, timeout)\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 911\u001B[0m     _, proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_direct_connection(req, traces, timeout)\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m proto\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/aiohttp/connector.py:1173\u001B[0m, in \u001B[0;36mTCPConnector._create_direct_connection\u001B[0;34m(self, req, traces, timeout, client_error)\u001B[0m\n\u001B[1;32m   1166\u001B[0m host_resolved \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resolve_host(host, port, traces\u001B[38;5;241m=\u001B[39mtraces), loop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loop\n\u001B[1;32m   1168\u001B[0m )\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1170\u001B[0m     \u001B[38;5;66;03m# Cancelling this lookup should not cancel the underlying lookup\u001B[39;00m\n\u001B[1;32m   1171\u001B[0m     \u001B[38;5;66;03m#  or else the cancel event will get broadcast to all the waiters\u001B[39;00m\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;66;03m#  across all connections.\u001B[39;00m\n\u001B[0;32m-> 1173\u001B[0m     hosts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mshield(host_resolved)\n\u001B[1;32m   1174\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mCancelledError:\n\u001B[1;32m   1176\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop_exception\u001B[39m(fut: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.Future[List[Dict[str, Any]]]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mCancelledError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "async def download_to_database(news_meta_path, news_meta_id, filename):\n",
    "    \n",
    "    if not os.path.isfile(f'articles/reuters/{filename}'):\n",
    "        url = f\"https://www.reuters.com{news_meta_path}\"\n",
    "        \n",
    "        headers = {\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',                                                                                                               \n",
    "            'accept-language': 'en-GB,en;q=0.9',\n",
    "            'cache-control': 'max-age=0',\n",
    "            'Cookie': '_ga_WBSR7WLTGD=GS1.1.1718979349.1.1.1718979470.2.0.0; usprivacy=1---; RT=\"z=1&dm=reuters.com&si=3oul8ffj52b&ss=lxorwub5&sl=0&tt=0\"; cleared-onetrust-cookies=Thu, 17 Feb 2022 19:17:07 GMT; _awl=2.1718979339.5-685e27cf11309595700479c50c4ae899-6763652d6575726f70652d7765737431-0; _ga=GA1.2.1619945513.1718979340; _gid=GA1.2.1048843721.1718979340; ajs_anonymous_id=317bfb4e-7e16-4e75-bd92-4c4b47428ac0; OptanonAlertBoxClosed=2024-06-21T14:15:40.868Z; eupubconsent-v2=CQAjn_AQAjn_AAcABBENA6EsAP_gAEPgACiQKIpB9G7eCSFDYHp3IJsEMIUH4Vho4sAgBhCBA4AAyBoQJIwGh2AwIACIICACGAIAIEYBIABgEAAAQEAAYIAAIABIAECEAQACIAAAAAIBAAAICAAoAAAAAAAAgFJEEhYBmAEAQBIAQNgAAgABATAQACAAAAAQAAAABAAEQAAAAwAAACAAkABAAAAAAAAAABkIAQBAAAAAQAAAABAAAAECAAAAAAIKCABkGjUCAFEQEhBIAEEAAEQQBAAQIAAAACBAgAASBgQJAwCFWAgAAAIAAAAAAAAIAABAAAIAAgAAEAAQIAAIAAIAAAAAAAAAAAAAAABAAAAAAAIAAAAAAAAAAJEAAQBgQBAABAAQFAAABAgABAAACABAAgQAAAAAAAAAAAAACAggAAAgAAAAAAAAAAAAAAIAABAAAAAAAAAAAABAAACAAAAQCAoAMADYAJwCEBAAgAOgCIAH_AakPABAAbABOOACAA6AIgAf8BqRIAIAA4AiAB_wGpEwAIAJygAYABwAOgCEAEQARqA1IqABABOA.f_wACHwAAAAA; OTAdditionalConsentString=1~70.89.196.311.385.407.413.445.523.584.1097.1421.1638.1703.1725.1859.2068.2072.2074.2213.2282.2309.2328.2337.2416.2418.2486.2567.2568.2571.2577.2628.2813.2822.2869.2909.2999.3000.3059.3100.3253.3331.9731.14332.15731.16931.27731.27831.28731.28831.29631.31631; OptanonConsent=isGpcEnabled=0&datestamp=Fri+Jun+21+2024+16%3A15%3A40+GMT%2B0200+(Central+European+Summer+Time)&version=202310.2.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=33f46668-8d0b-4bbd-8a04-90c69e88b8b1&interactionCount=1&landingPath=NotLandingPage&groups=1%3A1%2C2%3A1%2C3%3A1%2C4%3A1%2CV2STACK42%3A1; OneTrustWPCCPAGoogleOptOut=false; _gcl_au=1.1.1076428589.1718979342; permutive-id=37216e60-e9c9-41fc-995a-931fa7cda6d7; _scor_uid=fde365cfe44e41e1aed6c5d58be8ca23; _fbp=fb.1.1718979341841.66348021424789325; cookie=532c92d4-794a-41ba-b048-20a563775dc8; cookie_cst=4iwYLMgsTg%3D%3D; _lr_retry_request=true; _lr_env_src_ats=false; datadome=flRGV6V62rhDB6ibnCYnUnighMvhSZzcrS2jYhOLPsPSpepGIsTqY9YAVlYOdDVjBuIrpRZtrDtlP8EB2bowb_1anVbz0IMEsrKo2eFEy1P22AatRrjR44MyH3~DKOd_; __gads=ID=0c9c2b4c571a86ee:T=1718979352:RT=1718979352:S=ALNI_Mac_I_h4eH_DoNnUyu0JYEyS40nFg; __gpi=UID=00000e5f692313bc:T=1718979352:RT=1718979352:S=ALNI_MZcOfNKuHpNG0uzDo98pl9Bvkr5yQ; __eoi=ID=e291a31021f45927:T=1718979352:RT=1718979352:S=AA-AfjYcXJDHmvsm1n02b-mZC9N4; _au_1d=AU1D-0100-001718979354-8SMTV223-1SWW; _au_last_seen_iab_tcf=1718979354944; __qca=P0-364845901-1718979349757; _cc_id=743a0e863e36efa8c1a2edcb03eb70be; panoramaId_expiry=1719065754793; cto_bundle=VK8eW19NejNjcHl6MFRLSndUTTJoQkxrT08lMkZHRUJ0MUxCSVZBYWFkd1FlTEtmQVFRUVNpOXNRZEI1RlBCVTdVcmJ2Q0JscEF3aGVpNHpNOUpodE0zbFFsV1k0VWppY1RKUGdCcnFOUDNqd2luZmVmYWV2U1glMkZUZyUyRnR0ZTlVJTJGbXdjMkNW; cnx_userId=7cb41218569042f6a35b7e6e7b53f267; _gat=1; reuters-geo={\"country\":\"-\", \"region\":\"-\"}; _dd_s=rum=0&expire=1718980368091; datadome=Lyx4XFa9B5O7fxiFlEerx_X45~EsqiBWozyDnINV11oywjrE0D~yBHbooqIyD6l1uVSqo~FJ4~vfBfhDjfQxiA7yhGcoQ_ndauuE235uDPBBaJp~l7jG5~nIsmloxpyf; reuters-geo={\"country\":\"AT\", \"region\":\"-\"}',\n",
    "            'dnt': '1',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        async with aiohttp.ClientSession(headers=headers) as session:\n",
    "            print(f'Downloading {news_meta_id} , {url}')\n",
    "\n",
    "            async with session.get(url) as response:\n",
    "                try:\n",
    "                    response_text = await response.text()\n",
    "    \n",
    "                    # get content in article\n",
    "                    index_article_start = response_text.index(\"<article\")\n",
    "                    index_article_end = response_text.index(\"</article>\")\n",
    "                    article_text_html =  response_text[index_article_start:index_article_end+10]\n",
    "    \n",
    "                    bs = BeautifulSoup(article_text_html, \"xml\")\n",
    "    \n",
    "                    full_text = \"\"\n",
    "    \n",
    "                    for EachPart in bs.select('div[class*=\"article-body__content__\"]'):\n",
    "                        for paragraph in EachPart.parent.select('div[data-testid*=\"paragraph-\"]'):\n",
    "                            full_text += \" \" + paragraph.text\n",
    "    \n",
    "                    full_text = full_text.replace(\"\\n\",\"\")\n",
    "    \n",
    "                    with open(f'articles/reuters/{filename}.txt', 'w') as f:\n",
    "                        f.write(full_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading article {news_meta_id} | {response.status}\")\n",
    "\n",
    "async def load_article(search_term):\n",
    "    # for meta in meta_list:\n",
    "    \n",
    "    news_meta_list = newsMetaRepo.select_by_term(search_term)\n",
    "\n",
    "    def get_file_name(id):\n",
    "        title = news_meta_list['title'][id].replace(' ','-').replace(\"/\",\"-\")\n",
    "        return f\"{title}-{parser.parse(news_meta_list['timestamp'][id]).timestamp()}\"\n",
    "    \n",
    "    for i in range(0,len(news_meta_list)):\n",
    "        # response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "        try:\n",
    "            await download_to_database(\n",
    "                news_meta_list['url'][i],\n",
    "                news_meta_list['id'][i],\n",
    "                get_file_name(i)\n",
    "            )\n",
    "            sleep(0.500)\n",
    "\n",
    "        # task_list = [\n",
    "            #     asyncio.create_task(\n",
    "            #         download_to_database(\n",
    "            #             news_meta_list['url'][i+idx],\n",
    "            #             news_meta_list['id'][i+idx],\n",
    "            #             get_file_name(i+idx)\n",
    "            #         )\n",
    "            #     ) for idx in range(0,10 if len(news_meta_list) > i + 10 else len(news_meta_list) % 10)\n",
    "            # ]\n",
    "            # await asyncio.gather(*task_list)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR DOWNLOADING ... RETRYING...\")\n",
    "            # failed_tasks = [\n",
    "            #     asyncio.create_task(\n",
    "            #         download_to_database(\n",
    "            #             news_meta_list['url'][i+idx],\n",
    "            #             news_meta_list['id'][i+idx],\n",
    "            #             get_file_name(i+idx)\n",
    "            #         )\n",
    "            #     ) for idx in range(0,10 if len(news_meta_list) > i + 10 else len(news_meta_list) % 10)\n",
    "            # ]\n",
    "            # await asyncio.gather(*failed_tasks)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            sleep(3)\n",
    "\n",
    "        sleep(0.300)\n",
    "\n",
    "await load_article(\"Tesla\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:52.049557Z",
     "start_time": "2024-06-21T15:28:41.487585Z"
    }
   },
   "id": "bb14cc9bf67d17d0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='brightdata.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/urllib3/connectionpool.py:712\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_new_proxy_conn \u001B[38;5;129;01mand\u001B[39;00m http_tunnel_required:\n\u001B[0;32m--> 712\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_proxy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/urllib3/connectionpool.py:1012\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._prepare_proxy\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1010\u001B[0m     conn\u001B[38;5;241m.\u001B[39mtls_in_tls_required \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1012\u001B[0m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/urllib3/connection.py:374\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;66;03m# Calls self._set_hostport(), so self.host is\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;66;03m# self._tunnel_host below.\u001B[39;00m\n\u001B[0;32m--> 374\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tunnel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;66;03m# Mark this connection as not reusable\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/http/client.py:905\u001B[0m, in \u001B[0;36mHTTPConnection._tunnel\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    904\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 905\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTunnel connection failed: \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (code,\n\u001B[1;32m    906\u001B[0m                                                        message\u001B[38;5;241m.\u001B[39mstrip()))\n\u001B[1;32m    907\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[0;31mOSError\u001B[0m: Tunnel connection failed: 403 Forbidden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/urllib3/connectionpool.py:799\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    797\u001B[0m     e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, e)\n\u001B[0;32m--> 799\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    802\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/urllib3/util/retry.py:592\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_retry\u001B[38;5;241m.\u001B[39mis_exhausted():\n\u001B[0;32m--> 592\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause))\n\u001B[1;32m    594\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='brightdata.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mProxyError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Send a GET request to the website\u001B[39;00m\n\u001B[1;32m     20\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://brightdata.com/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 21\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Use BeautifulSoup to parse the HTML content of the website\u001B[39;00m\n\u001B[1;32m     24\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/adapters.py:513\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RetryError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, _ProxyError):\n\u001B[0;32m--> 513\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ProxyError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, _SSLError):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001B[39;00m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "\u001B[0;31mProxyError\u001B[0m: HTTPSConnectionPool(host='brightdata.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Define parameters provided by Brightdata\n",
    "host = 'brd.superproxy.io'\n",
    "port = 22225\n",
    "username = 'brd-customer-hl_3249edde-zone-datacenter_proxy1'\n",
    "password = 'xgej83znlhdn'\n",
    "session_id = random.random()\n",
    "\n",
    "# format your proxy\n",
    "proxy_url = ('http://{}-session-{}:{}@{}:{}'.format(username, session_id,\n",
    "                                                    password, host, port))\n",
    "\n",
    "# define your proxies in dictionary\n",
    "proxies = {'http': proxy_url, 'https': proxy_url}\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://brightdata.com/\"\n",
    "response = requests.get(url, proxies=proxies)\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML content of the website\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the links on the website\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# Print all the links\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T17:17:37.969480Z",
     "start_time": "2024-06-21T17:17:36.243384Z"
    }
   },
   "id": "20022cae0d517351",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-21T15:28:52.048629Z"
    }
   },
   "id": "64845b20b2eb2bbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
