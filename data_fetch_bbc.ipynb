{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BBC news Data fetch and save\n",
    "\n",
    "This notebook downloads all the data from the reuters about a specific topic and saves it to the database"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474db2cb8b2dfaf2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import ray\n",
    "from math import ceil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from http.client import HTTPSConnection\n",
    "from urllib.parse import urljoin, unquote\n",
    "import re\n",
    "from time import sleep\n",
    "import datetime\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os.path\n",
    "from dateutil import parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:02:49.102838Z",
     "start_time": "2024-06-21T08:02:48.676183Z"
    }
   },
   "id": "88818691fce235af",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "class NewsMetaRepository:\n",
    "    def __init__(self, csv_file=None):\n",
    "        self.connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "        if os.path.isfile(f'{csv_file}/news_meta.csv'):\n",
    "            try:\n",
    "                self.connection.execute(f\"IMPORT DATABASE '{csv_file}';\")\n",
    "            except Exception as e:\n",
    "                self._create_table()\n",
    "                self.connection.execute(f\"EXPORT DATABASE '{csv_file}';\")\n",
    "        else:\n",
    "            self._create_table()\n",
    "\n",
    "    def _create_table(self):\n",
    "        self.connection.execute(\"CREATE TABLE news_meta (id VARCHAR, title VARCHAR UNIQUE, url VARCHAR, timestamp VARCHAR UNIQUE,term VARCHAR)\")\n",
    "        self.connection.execute(\"CREATE SEQUENCE id_sequence START 1 INCREMENT BY 1;\")\n",
    "\n",
    "    def insert(self, news_meta):\n",
    "        self.connection.execute(\"PREPARE insert_meta AS \"\n",
    "                                \"INSERT INTO news_meta VALUES (nextval('id_sequence'), ?, ?, ?, ?) ON CONFLICT DO NOTHING;\")\n",
    "        self.connection.execute(f\"EXECUTE insert_meta('{news_meta['title']}', '{news_meta['url']}', '{news_meta['timestamp']}', '{news_meta['term']}');\")\n",
    "\n",
    "    def select_all(self):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta\").fetchdf()\n",
    "\n",
    "    def select_by_id(self, id):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE id = ?\", id).fetchdf()\n",
    "\n",
    "    def select_by_title(self, title):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE title = ?\", title).fetchdf()\n",
    "\n",
    "    def select_by_url(self, url):\n",
    "        return self.connection.execute(\"SELECT * FROM news_meta WHERE url = ?\", url).fetchdf()\n",
    "\n",
    "    def select_by_term(self, source):\n",
    "        self.connection.execute(\"PREPARE select_by_source AS \"\n",
    "                                \"SELECT * FROM news_meta WHERE term = ?\")\n",
    "        return self.connection.execute(f\"EXECUTE select_by_source('{source}');\").fetchdf()\n",
    "\n",
    "    def select_by_date(self, date_from, date_to):\n",
    "        return self.connection.execute(f\"SELECT * FROM news_meta WHERE strptime(timestamp, '%Y-%m-%dT%H:%M:%S%z') BETWEEN strptime('{date_from}','%Y-%m-%d') AND strptime('{date_to}','%Y-%m-%d')\").fetchdf()\n",
    "\n",
    "    def delete_all(self):\n",
    "        self.connection.execute(\"DELETE FROM news_meta\")\n",
    "\n",
    "    def export(self, csv_file):\n",
    "        self.connection.execute(f\"EXPORT DATABASE '{csv_file}';\")\n",
    "\n",
    "    def close(self):\n",
    "        self.connection.close()\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:02:49.863511Z",
     "start_time": "2024-06-21T08:02:49.855947Z"
    }
   },
   "id": "165e2f88bb4c18f1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def try_request(url, headers, params, isJson=True):\n",
    "\n",
    "    # resp_proxy = requests.get('https://free-proxy-list.net/')\n",
    "    # df = pd.read_html(resp_proxy.text)[0]\n",
    "    # df_http = df[df['Https']=='no']\n",
    "    # df_https = df[df['Https']=='yes']\n",
    "    # proxy_http = f'https://{df_http[\"IP Address\"].values[0]}:{df_http[\"Port\"].values[0]}'\n",
    "    # proxy_https = f'https://{df_https[\"IP Address\"].values[0]}:{df_https[\"Port\"].values[0]}'\n",
    "\n",
    "    proxies={\n",
    "        \"http\": 'socks5://193.35.18.30:30808',\n",
    "        \"https\": 'socks5://193.35.18.30:30808'\n",
    "    }\n",
    "\n",
    "    response_page = requests.request(\"GET\", url, headers=headers, params=params, timeout=5)\n",
    "    try:\n",
    "        response = response_page.json() if isJson else response_page.text\n",
    "\n",
    "        if response_page.status_code > 300:\n",
    "            print(\"Status code error: \" + str(response_page.status_code))\n",
    "            return False, params\n",
    "\n",
    "        return True, response\n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(f\"Bad Request: GET {url} \\n Status Code: {response_page.status_code} | Error : {e}\")\n",
    "        return False, params\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timed out\")\n",
    "        return False, params\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Bad Request: GET {url} \\n\")\n",
    "        return False, params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:02:50.890101Z",
     "start_time": "2024-06-21T08:02:50.885458Z"
    }
   },
   "id": "c28d85168b8a0aa5",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page 0\n",
      "Loading page 1\n",
      "Loading page 2\n",
      "Loading page 3\n",
      "Loading page 4\n",
      "Loading page 5\n",
      "Loading page 6\n",
      "Loading page 7\n",
      "Loading page 8\n",
      "Loading page 9\n",
      "Loading page 10\n",
      "Loading page 11\n",
      "Loading page 12\n",
      "Loading page 13\n",
      "Loading page 14\n",
      "Loading page 15\n",
      "Done loading.\n"
     ]
    }
   ],
   "source": [
    "newsMetaRepo = NewsMetaRepository(\"meta/bbc\")\n",
    "\n",
    "def create_request_url(term, page, page_size):\n",
    "    template = 'https://web-cdn.api.bbci.co.uk/xd/search?terms={}?page={}?pageSize={}'\n",
    "    url = template.format(term, page, page_size)\n",
    "    return url\n",
    "\n",
    "bbc_url = 'https://web-cdn.api.bbci.co.uk/xd/search'\n",
    "\n",
    "def get_news_meta(search_term):\n",
    "    hasData = True\n",
    "    page = 0\n",
    "    \n",
    "    while hasData:\n",
    "        print(f'Loading page {page}')\n",
    "\n",
    "        response_ok, response = try_request(\n",
    "            url = bbc_url,\n",
    "            headers={},\n",
    "            params = {\n",
    "                'terms': search_term,\n",
    "                'page': page,\n",
    "                'pageSize': 100\n",
    "            }\n",
    "        )\n",
    "                \n",
    "        if response_ok:            \n",
    "            if len(response[\"data\"]) > 0:\n",
    "                for data in response[\"data\"]:\n",
    "                    news_meta = {\n",
    "                        'title': data['title'].replace(\"'\",\"''\"),\n",
    "                        'term': search_term,\n",
    "                        'timestamp': data['firstPublishedAt'], \n",
    "                        'url': data['path']\n",
    "                    }\n",
    "                    newsMetaRepo.insert(news_meta) \n",
    "            else: \n",
    "                print(\"Done loading.\")\n",
    "                hasData = False\n",
    "        \n",
    "        newsMetaRepo.export('meta/bbc')\n",
    "        page += 1\n",
    "\n",
    "get_news_meta(\"Tesla\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:02:57.994577Z",
     "start_time": "2024-06-21T08:02:52.527103Z"
    }
   },
   "id": "294aa17dad924fb8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "async def download_article(news_meta_url, news_meta_id, news_meta_filename):\n",
    "    file_name = f'articles/bbc/{news_meta_filename}.txt'\n",
    "    if not os.path.isfile(file_name):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            print(f'Downloading {news_meta_id} , https://bbc.com{news_meta_url}')\n",
    "    \n",
    "            async with session.get(f'https://bbc.com{news_meta_url}') as response: \n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "        \n",
    "                complete_text = ''\n",
    "        \n",
    "                if soup.find('article') is not None:\n",
    "                    text_blocks = soup.find('article').find_all('div', {'data-component':\"text-block\"})\n",
    "                    for text_block in text_blocks:\n",
    "                        text_paragraphs = text_block.find_all('p')\n",
    "                        complete_text += ' '.join([paragraph.text for paragraph in text_paragraphs])\n",
    "                else:\n",
    "                    video_type = soup.find('div',{'data-testid':'video-page-video-section'})\n",
    "                    if video_type is not None:\n",
    "                        paragraphs = video_type.find_all('p')\n",
    "                        complete_text += ' '.join([paragraph.text for paragraph in paragraphs])\n",
    "                    \n",
    "                with open(f'articles/bbc/{news_meta_filename}.txt', 'w') as f:\n",
    "                    f.write(complete_text)\n",
    "\n",
    "\n",
    "async def get_news_articles(source):\n",
    "    news_meta_list = newsMetaRepo.select_by_term(source)\n",
    "    # for i in range(0, 200): #len(news_meta_list)\n",
    "    #     print(f'Downloading {news_meta_list[\"id\"][i]} , https://bbc.com{news_meta_list[\"url\"][i]}')\n",
    "    #     download_article(news_meta_list['url'][i],news_meta_list['id'][i])\n",
    "    #     sleep(0.35)\n",
    "\n",
    "    def get_file_name(id):\n",
    "        title = news_meta_list['title'][id].replace(' ','-').replace(\"/\",\"-\")\n",
    "        return f\"{title}-{parser.parse(news_meta_list['timestamp'][id]).timestamp()}\"\n",
    "\n",
    "    for i in range(0, len(news_meta_list))[::10]:\n",
    "        print(i)\n",
    "        try:\n",
    "            task_list = [\n",
    "                asyncio.create_task(\n",
    "                    download_article(\n",
    "                        news_meta_list['url'][i+idx],\n",
    "                        news_meta_list['id'][i+idx],\n",
    "                        get_file_name(i+idx)\n",
    "                    )\n",
    "                ) for idx in range(0,10 if len(news_meta_list) > i + 10 else len(news_meta_list) % 10)\n",
    "            ]\n",
    "            await asyncio.gather(*task_list)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR DOWNLOADING ... RETRYING...\")\n",
    "            failed_tasks = [\n",
    "                asyncio.create_task(\n",
    "                    download_article(\n",
    "                        news_meta_list['url'][i+idx],\n",
    "                        news_meta_list['id'][i+idx],\n",
    "                        get_file_name(i+idx)\n",
    "                    )\n",
    "                ) for idx in range(0,10 if len(news_meta_list) > i + 10 else len(news_meta_list) % 10)\n",
    "            ]\n",
    "            await asyncio.gather(*failed_tasks)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            sleep(3)\n",
    "\n",
    "        sleep(0.300)\n",
    "    # \n",
    "    # if len(failed_tasks) > 0:\n",
    "    #     print(f'\\n\\nLength of the failed task:  {len(failed_tasks)}')\n",
    "    #     # load failed tasks\n",
    "    #     for i in range(0,len(failed_tasks))[::10]:\n",
    "    #         print(\"Loading\")\n",
    "    #         await asyncio.gather(*(failed_tasks[i:i+10]))\n",
    "    #         sleep(2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:03:04.775489Z",
     "start_time": "2024-06-21T08:03:04.770941Z"
    }
   },
   "id": "3f5e71090d4c9aa4",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page 0\n",
      "Loading page 1\n",
      "Loading page 2\n",
      "Loading page 3\n",
      "Loading page 4\n",
      "Loading page 5\n",
      "Loading page 6\n",
      "Loading page 7\n",
      "Loading page 8\n",
      "Loading page 9\n",
      "Loading page 10\n",
      "Loading page 11\n",
      "Loading page 12\n",
      "Loading page 13\n",
      "Loading page 14\n",
      "Loading page 15\n",
      "Done loading.\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n"
     ]
    }
   ],
   "source": [
    "company = \"Tesla\"\n",
    "\n",
    "get_news_meta(company)\n",
    "await get_news_articles(company)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:05:52.434660Z",
     "start_time": "2024-06-21T08:03:38.884099Z"
    }
   },
   "id": "a7742534cf8df85b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e3b28147f5deb333"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
