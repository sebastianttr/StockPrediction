{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reuters Data Collection & Processing\n",
    "\n",
    "This notebook serve the purpose to fetch the data from the reuters webpage using their query api and crawling methods\n",
    "\n",
    "Disclaimer: Crawler is legal here. If the user can see the data without boting (login), then you can fetch it with normal requests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bc7557e430f687"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f83e0ae0fe0a9fad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import requests\n",
    "from pygments.lexer import combined\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:44:28.537360Z",
     "start_time": "2024-02-21T18:44:28.517876Z"
    }
   },
   "id": "e68ef8635b5b495f",
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constant params "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1fa951381c77a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_PAGE_SIZE = 100\n",
    "REUTERS_QUERY_URL = 'https://www.reuters.com/pf/api/v3/content/fetch/articles-by-search-v2'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:44:28.538327Z",
     "start_time": "2024-02-21T18:44:28.525689Z"
    }
   },
   "id": "c88daad598a6354d",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/models.py:971\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplexjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/json/__init__.py:357\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    355\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    356\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 47\u001B[0m\n\u001B[1;32m     43\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeyword\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:0,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morderby\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay_date:desc\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwebsite\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreuters\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (keyword,MAX_PAGE_SIZE)\n\u001B[1;32m     44\u001B[0m         response_page \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m,REUTERS_QUERY_URL)\n\u001B[0;32m---> 47\u001B[0m \u001B[43mquery_reuters_news_meta\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTesla\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[101], line 25\u001B[0m, in \u001B[0;36mquery_reuters_news_meta\u001B[0;34m(keyword)\u001B[0m\n\u001B[1;32m     18\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeyword\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:0,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morderby\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay_date:desc\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwebsite\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreuters\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (keyword,MAX_PAGE_SIZE),\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m174\u001B[39m,\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_website\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreuters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     22\u001B[0m }\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# make the first request\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m response_first \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mREUTERS_QUERY_URL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# check if the request was valid by the given status code, else raise exception\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response_first[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatusCode\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/thesisenv/lib/python3.8/site-packages/requests/models.py:975\u001B[0m, in \u001B[0;36mResponse.json\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m complexjson\u001B[38;5;241m.\u001B[39mloads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n\u001B[0;32m--> 975\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RequestsJSONDecodeError(e\u001B[38;5;241m.\u001B[39mmsg, e\u001B[38;5;241m.\u001B[39mdoc, e\u001B[38;5;241m.\u001B[39mpos)\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:44:29.191494Z",
     "start_time": "2024-02-21T18:44:28.531104Z"
    }
   },
   "id": "b7e8edb5cefd3cc7",
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get keywords related to a company.\n",
    "\n",
    "## Get entities from the text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6504fe1d00ab70f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "text = \"\"\"\n",
    "Tesla, Inc. is an American multinational automotive and clean energy company headquartered in Austin, Texas, which designs, manufactures and sells electric vehicles, stationary battery energy storage devices from home to grid-scale, solar panels and solar shingles, and related products and services.\n",
    "Tesla was incorporated in July 2003 by Martin Eberhard and Marc Tarpenning as Tesla Motors. The company's name is a tribute to inventor and electrical engineer Nikola Tesla. In February 2004 Elon Musk joined as the company's largest shareholder and in 2008 he was named CEO. In 2008, the company began production of its first car model, the Roadster sports car, followed by the Model S sedan in 2012, the Model X SUV in 2015, the Model 3 sedan in 2017, the Model Y crossover in 2020, the Tesla Semi truck in 2022 and the Cybertruck pickup truck in 2023. The Model 3 is the all-time bestselling plug-in electric car worldwide, and in June 2021 became the first electric car to sell 1 million units globally. In 2023, the Model Y was the best-selling vehicle, of any kind, globally.Tesla is one of the world's most valuable companies. In October 2021, Tesla's market capitalization temporarily reached $1 trillion, the sixth company to do so in U.S. history. As of 2023, it is the world's most valuable automaker. In 2022, the company led the battery electric vehicle market, with 18% share.\n",
    "Tesla has been the subject of lawsuits, government scrutiny, and journalistic criticism, stemming from allegations of whistleblower retaliation, worker rights violations, product defects, and Musk's many controversial statements.\n",
    "\"\"\"\n",
    "\n",
    "# convert the wordpiece into one\n",
    "def combine_bert_word_piece(doc, entities):    \n",
    "    for entity in entities:\n",
    "        if entity['word'].startswith(\"#\"):\n",
    "        \n",
    "            # find the entity before \n",
    "            before_entities = [*filter(lambda ent: ent[\"end\"] == entity[\"start\"], entities)]\n",
    "            \n",
    "            # if the port of the word is found\n",
    "            if len(before_entities) > 0:\n",
    "                # get the entity\n",
    "                before_entity = before_entities[0]\n",
    "                \n",
    "                # check if the combination exists in the text \n",
    "                combined = (before_entity[\"word\"] + entity[\"word\"].replace(\"##\",\"\"))\n",
    "                                \n",
    "                # if the combination exists in the document, get it \n",
    "                if combined in doc:\n",
    "                    # remove both entites which are not relevant for further processing\n",
    "                    entities = [*filter(lambda ent: ent[\"end\"] != entity[\"start\"] and entity[\"word\"] != ent[\"word\"],entities)]\n",
    "                    \n",
    "                    # replace it with a word relevant to the t'ext\n",
    "                    entity[\"word\"] = combined\n",
    "                    entities.append(entity)\n",
    "            else:       # if the part does not have a before part, remove it \n",
    "                entities = [*filter(lambda ent: entity['word'] != ent[\"word\"],entities)]\n",
    "                \n",
    "    return entities\n",
    "\n",
    "def remove_duplicates_entities(entities): \n",
    "    new_entities = []\n",
    "    for entity in entities:\n",
    "        word = entity[\"word\"]\n",
    "        if word not in new_entities:\n",
    "            new_entities.append(word)\n",
    "            \n",
    "    return new_entities\n",
    "\n",
    "# leave only the misc in the text\n",
    "ner_entities = nlp(text)\n",
    "# print(ner_entities)\n",
    "\n",
    "# combine the remaining bert word pieces if any are left out\n",
    "ner_entities = combine_bert_word_piece(text, ner_entities)\n",
    "\n",
    "# remove duplicates entities\n",
    "ner_entities = remove_duplicates_entities(ner_entities)\n",
    "\n",
    "print(ner_entities)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T18:44:29.191409Z"
    }
   },
   "id": "1e5dec7e97152aef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get keywords from the text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f4822a4bc6e1c74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesla, Inc', 'American', 'Austin', 'Texas', 'Tesla', 'Martin Eberhard', 'Marc Tarp', 'Tesla Motors', 'Nikola Tesla', 'Roadster', 'Model S', 'Model X', 'Model 3', 'Model Y', 'Cy', 'U. S.', 'Musk', 'Elon Musk', 'Tesla Semi', 'Tesla, Inc.', 'clean energy', 'Austin, Texas', 'stationary battery', 'energy', 'solar panels', 'Marc Tarpenning', 'sports car', 'Semi truck', 'pickup truck', 'plug -', 'car', 'electric']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_keyword = AutoTokenizer.from_pretrained(\"yanekyuk/bert-keyword-extractor\")\n",
    "model_keyword = AutoModelForTokenClassification.from_pretrained(\"yanekyuk/bert-keyword-extractor\")\n",
    "\n",
    "nlp_keyword = pipeline(\"token-classification\", model=model_keyword, tokenizer=tokenizer_keyword, grouped_entities=True)\n",
    " \n",
    "# get all keywords from the text\n",
    "\n",
    "def get_keywords_from_text(txt):\n",
    "    res = combine_bert_word_piece(text,nlp_keyword(txt))\n",
    "    res = [*map(lambda k_word: k_word[\"word\"], res)]\n",
    "    return res\n",
    "\n",
    "keyword_result = get_keywords_from_text(text)\n",
    "\n",
    "key_tokens = ner_entities.copy()\n",
    "\n",
    "# join with entities list\n",
    "def join_key_token_lists(token_list_a, token_list_b):\n",
    "    for keyword in token_list_a:\n",
    "        if keyword not in token_list_b:\n",
    "            token_list_b.append(keyword)\n",
    "\n",
    "    return token_list_b\n",
    "\n",
    "keyword_result = join_key_token_lists(keyword_result,key_tokens)\n",
    "\n",
    "print(keyword_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:46:39.211165Z",
     "start_time": "2024-02-21T18:46:33.914524Z"
    }
   },
   "id": "c6d03ab3628f1aac",
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single Function to extract from wikipedia page"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277691783cf67139"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_wikipedia_sections(wikipedia):\n",
    "    sections_content = wikipedia_sections(wikipedia.sections,0,[] , {})    \n",
    "    sections_content[\"main\"] = wikipedia.summary\n",
    "    return sections_content\n",
    "\n",
    "def get_wikipedia_full_page(wikipedia):\n",
    "    sections_content = wikipedia_sections(wikipedia.sections,0,[] , {})\n",
    "    text_full = []\n",
    "    for section in sections_content:\n",
    "        text_full.append(sections_content[section])\n",
    "    return text_full\n",
    "\n",
    "def wikipedia_sections(sections, level, sections_list, out):\n",
    "    # get all the sections \n",
    "    for s in sections:\n",
    "        if s.title not in out:\n",
    "            out[s.title] = s.text\n",
    "        \n",
    "        wikipedia_sections(s.sections, level + 1, sections_list, out)\n",
    "        \n",
    "    return out\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:46:41.107598Z",
     "start_time": "2024-02-21T18:46:41.101877Z"
    }
   },
   "id": "29e3f68a64439105",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': ['Tesla, Inc', 'American', 'Austin', 'Texas', 'Tesla', 'Martin Eberhard', 'Marc Tarp', 'Tesla Motors', 'Nikola Tesla', 'Roadster', 'Model S', 'Model X', 'Model 3', 'Model Y', 'Tesla Semi', 'Cy', 'U. S.', 'Musk', 'Elon Musk'], 'keywords': ['Tesla', '.', 'clean energy', 'Austin, Texas', 'stationary battery', 'energy storage', 'Martin Eberhard', 'Marc Tarpenning', 'Nikola Tesla', 'Elon Musk', 'Roadster', 'sports car', 'Semi truck', 'plug -', 'electric', 'Tesla, Inc', 'American', 'Austin', 'Texas', 'Marc Tarp', 'Tesla Motors', 'Model S', 'Model X', 'Model 3', 'Model Y', 'Tesla Semi', 'Cy', 'U. S.', 'Musk'], 'topics': [], 'combined': ['Tesla', '.', 'clean energy', 'Austin, Texas', 'stationary battery', 'energy storage', 'Martin Eberhard', 'Marc Tarpenning', 'Nikola Tesla', 'Elon Musk', 'Roadster', 'sports car', 'Semi truck', 'plug -', 'electric', 'Tesla, Inc', 'American', 'Austin', 'Texas', 'Marc Tarp', 'Tesla Motors', 'Model S', 'Model X', 'Model 3', 'Model Y', 'Tesla Semi', 'Cy', 'U. S.', 'Musk']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_duplicate_keywords(keywords):\n",
    "    new_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if keyword not in new_keywords:\n",
    "            new_keywords.append(keyword)\n",
    "\n",
    "    return new_keywords\n",
    "\n",
    "def get_wikipedia_page(name):\n",
    "    # get wikipedia page content\n",
    "    wiki_page = wikipediaapi.Wikipedia('Sebastian Tatar (sebi.tatar2@gmail.com)', 'en')\n",
    "    return wiki_page.page(name)\n",
    "\n",
    "# get keywords from wikipedia page\n",
    "def get_relevant_words_from_company(name):\n",
    "    \n",
    "    # get wikipedia page content\n",
    "    wiki_page = get_wikipedia_page(name)\n",
    "   \n",
    "    sections = get_wikipedia_sections(wiki_page)\n",
    "    section_main = sections[\"main\"]\n",
    "            \n",
    "    # get the name entity\n",
    "    named_entities = nlp(section_main)\n",
    "    # combine the remaining bert word pieces if any are left out\n",
    "    named_entities = combine_bert_word_piece(example, named_entities)\n",
    "    \n",
    "    # remove duplicates entities\n",
    "    named_entities = remove_duplicates_entities(named_entities)\n",
    "    \n",
    "    # get company keywords\n",
    "    keywords = get_keywords_from_text(section_main)\n",
    "    #print(keywords)\n",
    "    keywords = remove_duplicate_keywords(keywords)\n",
    "    \n",
    "    # TODO: Topic Modeling\n",
    " \n",
    "    return {\n",
    "        \"entities\": named_entities,\n",
    "        \"keywords\": keywords,\n",
    "        \"topics\":[],\n",
    "        \"combined\": join_key_token_lists(named_entities,keywords),\n",
    "    }\n",
    "\n",
    "keywords_tesla = get_relevant_words_from_company(\"Tesla, Inc.\")\n",
    "#keywords_nvidia = get_relevant_words_from_company(\"Nvidia\")\n",
    "#keywords_microsoft = get_relevant_words_from_company(\"Microsoft\")\n",
    "#keywords_proctor_gamble = get_relevant_words_from_company(\"Procter & Gamble\")\n",
    "#keywords_coca_cola = get_relevant_words_from_company(\"Coca-Cola\")\n",
    "\n",
    "print(keywords_tesla)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:46:59.564740Z",
     "start_time": "2024-02-21T18:46:51.413884Z"
    }
   },
   "id": "72c786ba7de2cd48",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "keywords_tfidf = keywords_tesla.copy()\n",
    "\n",
    "def get_keywords_weight_from_docs(documents, keywords):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit_transform(keywords.apply(lambda x: \" \".join(x))).toarray()\n",
    "\n",
    "    print(res[1])\n",
    "    \n",
    "wiki_page_tesla = get_wikipedia_page(\"Tesla, Inc.\")\n",
    "documents_tesla = get_wikipedia_full_page(wiki_page_tesla)[1:] # first is empty\n",
    "\n",
    "get_keywords_weight_from_docs(documents_tesla, keywords_tesla)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:54:18.959789Z",
     "start_time": "2024-02-21T18:54:18.503602Z"
    }
   },
   "id": "bc3ce62b092e57e4",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T18:44:29.200965Z"
    }
   },
   "id": "c9ce4d93e01f0dd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
